{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitmyenvvenv8a7a196658e84f2b966e0f9169252f8b",
   "display_name": "Python 3.7.7 64-bit ('myenv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from  torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.optim\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms=transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading requried data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=datasets.MNIST('data',train=True,download=True,transform=transforms)\n",
    "test_data=datasets.MNIST('data',train=False,download=True,transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_size=0.2\n",
    "\n",
    "# IMPORTANT \n",
    "batch_size=20\n",
    "\n",
    "num_workers=0\n",
    "\n",
    "train_length = len(train_data)\n",
    "\n",
    "# obtain training dataset indices that will be used for validation dataset\n",
    "indices = list(range(train_length))\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * train_length))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders for train, test and validation dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the length of our train, valid and test dataloaders\n",
    "# NOTE : Here batch size is 20\n",
    "len(train_loader),len(valid_loader),len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking our data\n",
    "dataiter=iter(train_loader)\n",
    "images,labels=dataiter.next()\n",
    "print(images, images.shape, len(images), images[0].shape)\n",
    "print()\n",
    "print(labels,labels.shape,len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Training batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying images and labels of a batch\n",
    "fig=plt.figure(figsize=(30,10))\n",
    "for i in range(len(labels)):\n",
    "    ax=fig.add_subplot(2,10,i+1,xticks=[],yticks=[])\n",
    "    plt.imshow(np.squeeze(images[i]))\n",
    "    ax.set_title(labels[i].item(),color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F_Auto_MNIST(nn.Module):\n",
    "    def __init__(self,en_dim):\n",
    "        super(F_Auto_MNIST,self).__init__()\n",
    "        self.encoder=nn.Linear(784,en_dim)\n",
    "        self.decoder=nn.Linear(en_dim,784)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.encoder(x))\n",
    "        x=F.sigmoid(self.decoder(x))\n",
    "        return x\n",
    "\n",
    "class Tran_conv_Auto_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tran_conv_Auto_MNIST,self).__init__()\n",
    "        self.en_conv1=nn.Conv2d(1,16,3,padding=1)\n",
    "        self.en_conv2=nn.Conv2d(16,4,3,padding=1)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.de_conv1=nn.ConvTranspose2d(4,16,2,stride=2)\n",
    "        self.de_conv2=nn.ConvTranspose2d(16,1,2,stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.en_conv1(x)))\n",
    "        x=self.pool(F.relu(self.en_conv2(x)))\n",
    "        x=F.relu(self.de_conv1(x))\n",
    "        x=F.sigmoid(self.de_conv2(x))\n",
    "        return x\n",
    "\n",
    "class upsamp_conv_Auto_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(upsamp_conv_Auto_MNIST,self).__init__()\n",
    "        self.en_conv1=nn.Conv2d(1,16,3,padding=1)\n",
    "        self.en_conv2=nn.Conv2d(16,4,3,padding=1)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.de_conv1=nn.Conv2d(4,16,3,padding=1)\n",
    "        self.de_conv2=nn.Conv2d(16,1,3,padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.en_conv1(x)))\n",
    "        x=self.pool(F.relu(self.en_conv2(x)))\n",
    "        x = F.upsample(x, scale_factor=2, mode='nearest')\n",
    "        x=F.relu(self.de_conv1(x))\n",
    "        x = F.upsample(x, scale_factor=2, mode='nearest')\n",
    "        x=F.sigmoid(self.de_conv2(x))\n",
    "        return x\n",
    "\n",
    "#Since the images are normalized between 0 and 1, we need to use a sigmoid activation on the output layer to get values that match this input value range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_1=F_Auto_MNIST(32)\n",
    "model_2=Tran_conv_Auto_MNIST()\n",
    "model_3=upsamp_conv_Auto_MNIST()\n",
    "\n",
    "def weight_init_normal(m):\n",
    "    classname=m.__class__.__name__\n",
    "    if classname.find('Linear')!=-1:\n",
    "        n = m.in_features\n",
    "        y = (1.0/np.sqrt(n))\n",
    "        m.weight.data.normal_(0, y)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "model_1.apply(weight_init_normal)\n",
    "\n",
    "use_cuda=True\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "  model_1.cuda()\n",
    "  model_2.cuda()\n",
    "  model_3.cuda()\n",
    "print(model_1,'\\n\\n\\n\\n',model_2,'\\n\\n\\n\\n',model_3,'\\n\\n\\n\\n','On GPU : ',torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used when comapring pixel values.\n",
    "criterion=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(model,lr,state='fully'):\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "    # Number of epochs to train for\n",
    "    loss_keeper={'train':[],'valid':[]}\n",
    "    epochs=20\n",
    "\n",
    "    # minimum validation loss ----- set initial minimum to infinity\n",
    "    valid_loss_min = np.Inf \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss=0.0\n",
    "        valid_loss=0.0\n",
    "\n",
    "        \"\"\"\n",
    "        TRAINING PHASE\n",
    "        \"\"\"\n",
    "        model.train() # TURN ON DROPOUT for training\n",
    "        for images,_ in train_loader:\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                images=images.cuda()\n",
    "            if state=='fully':\n",
    "                images=images.view(images.size(0),-1)\n",
    "            optimizer.zero_grad()\n",
    "            output=model(images)\n",
    "            loss=criterion(output,images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "\n",
    "        \"\"\"\n",
    "        VALIDATION PHASE\n",
    "        \"\"\"\n",
    "        model.eval() # TURN OFF DROPOUT for validation\n",
    "        for images,_ in valid_loader:\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                images=images.cuda()\n",
    "            if state=='fully':\n",
    "                images=images.view(images.size(0),-1)\n",
    "            output=model(images)\n",
    "            loss=criterion(output,images)\n",
    "            valid_loss+=loss.item()\n",
    "\n",
    "        # Calculating loss over entire batch size for every epoch\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        valid_loss = valid_loss/len(valid_loader)\n",
    "\n",
    "\n",
    "        # saving loss values\n",
    "        loss_keeper['train'].append(train_loss)\n",
    "        loss_keeper['valid'].append(valid_loss)\n",
    "\n",
    "        print(f\"\\nEpoch : {epoch+1}\\tTraining Loss : {train_loss}\\tValidation Loss : {valid_loss}\")\n",
    "        if valid_loss<=valid_loss_min:\n",
    "            print(f\"Validation loss decreased from : {valid_loss_min} ----> {valid_loss} ----> Saving Model.......\")\n",
    "            z=type(model).__name__\n",
    "            torch.save(model.state_dict(), z+'_model.pth')\n",
    "            valid_loss_min=valid_loss\n",
    "    return(loss_keeper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m1_loss=trainNet(model_1,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m2_loss=trainNet(model_2,0.01,'conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m3_loss=trainNet(model_3,0.01,'conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.load_state_dict(torch.load('F_Auto_MNIST_model.pth'))\n",
    "model_2.load_state_dict(torch.load('Tran_conv_Auto_MNIST_model.pth'))\n",
    "model_3.load_state_dict(torch.load('upsamp_conv_Auto_MNIST_model.pth'))"
   ]
  },
  {
   "source": [
    "## Plotting Loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=['FFNN','Transpose CNN','Upsampled CNN']\n",
    "model_losses=[m1_loss,m2_loss,m3_loss]\n",
    "\n",
    "fig=plt.figure(1,figsize=(25,5))\n",
    "idx=1\n",
    "for i in model_losses:\n",
    "  ax=fig.add_subplot(1,3,idx)\n",
    "  ax.plot(i['train'],label=\"Training Loss\")\n",
    "  ax.plot(i['valid'],label=\"Validation Loss\")\n",
    "  ax.set_title('MNIST Autoencoder: '+title[idx-1])\n",
    "  idx+=1\n",
    "  plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,state='fully'):\n",
    "    # obtain one batch of test images\n",
    "    dataiter = iter(test_loader)\n",
    "    images, _ = dataiter.next()\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        images=images.cuda()\n",
    "    if state=='fully':\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "    # get sample outputs\n",
    "    output = model(images)\n",
    "    # prep images for display\n",
    "    images = images.cpu().numpy()\n",
    "    # output is resized into a batch of images\n",
    "    output = output.view(batch_size, 1, 28, 28)\n",
    "    # use detach when it's an output that requires_grad\n",
    "    output = output.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    # plot the first ten input images and then reconstructed images\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))\n",
    "\n",
    "    # input images on top row, reconstructions on bottom\n",
    "    for images, row in zip([images, output], axes):\n",
    "        for img, ax in zip(images, row):\n",
    "            if state=='fully':\n",
    "                img = img.reshape(1, 28, 28)\n",
    "            ax.imshow(np.squeeze(img), cmap='gray')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            "
   ]
  },
  {
   "source": [
    "## FFNN Autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(model_1)"
   ]
  },
  {
   "source": [
    "## Transpose CNN Autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test(model_2,'conv')"
   ]
  },
  {
   "source": [
    "## Upsampled CNN Autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model_3,'conv')"
   ]
  }
 ]
}